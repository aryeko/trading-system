# Copyright (c) 2025 Arye Kogan
# SPDX-License-Identifier: MIT

"""Preprocessing pipeline that transforms raw bars into curated datasets."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from datetime import date
from pathlib import Path

import numpy as np
import pandas as pd

from trading_system.config import Config, PreprocessConfig
from trading_system.data import BARS_COLUMN_ORDER, ensure_bars_frame

logger = logging.getLogger(__name__)

REQUIRED_RAW_COLUMNS: tuple[str, ...] = BARS_COLUMN_ORDER

CANONICAL_COLUMNS: list[str] = list(BARS_COLUMN_ORDER) + [
    "sma_100",
    "sma_200",
    "ret_1d",
    "ret_20d",
    "rolling_peak",
]


@dataclass(frozen=True, slots=True)
class PreprocessResult:
    """Summary of artifacts generated by the preprocessor."""

    as_of: date
    symbols: tuple[str, ...]
    artifacts: dict[str, Path]


class Preprocessor:
    """Transform raw bars into curated datasets with derived indicators."""

    def __init__(self, config: Config) -> None:
        self._config = config
        self._policy = config.preprocess or PreprocessConfig()
        self._source_dir = config.paths.data_raw
        self._target_dir = config.paths.data_curated
        self._adjust_policy = config.data.adjust

    def run(self, as_of: date | str | pd.Timestamp) -> PreprocessResult:
        """Process raw parquet inputs for ``as_of`` into curated parquet outputs."""

        as_of_ts = _normalize_timestamp(as_of)
        as_of_str = as_of_ts.strftime("%Y-%m-%d")

        raw_dir = self._source_dir / as_of_str
        if not raw_dir.is_dir():
            raise FileNotFoundError(f"Raw data directory not found: {raw_dir}")

        curated_dir = self._target_dir / as_of_str
        curated_dir.mkdir(parents=True, exist_ok=True)

        artifacts: dict[str, Path] = {}
        symbols: list[str] = []

        for file_path in sorted(raw_dir.glob("*.parquet")):
            data = pd.read_parquet(file_path)
            symbol = _infer_symbol(file_path, data)
            self._validate_raw_columns(data, symbol)
            curated = self._curate_frame(data, symbol, as_of_ts)
            output_path = curated_dir / f"{symbol}.parquet"
            curated.to_parquet(output_path, index=False)
            artifacts[symbol] = output_path
            symbols.append(symbol)

        return PreprocessResult(
            as_of=as_of_ts.date(),
            symbols=tuple(sorted(symbols)),
            artifacts=artifacts,
        )

    def _validate_raw_columns(self, frame: pd.DataFrame, symbol: str) -> None:
        missing = [
            column for column in REQUIRED_RAW_COLUMNS if column not in frame.columns
        ]
        if missing:
            missing_list = ", ".join(missing)
            raise ValueError(
                f"Raw data for {symbol} missing required columns: {missing_list}"
            )

    def _curate_frame(
        self, frame: pd.DataFrame, symbol: str, as_of: pd.Timestamp
    ) -> pd.DataFrame:
        data = ensure_bars_frame(frame)
        data["date"] = pd.to_datetime(data["date"], utc=False)
        data = data.sort_values("date")

        if "adj_close" not in data.columns:
            data["adj_close"] = data["close"]

        data = self._apply_adjustments(data)

        data = data.set_index("date")
        index = pd.DatetimeIndex(data.index)
        if index.tz is not None:
            index = index.tz_convert(None)
        data.index = index

        start = data.index.min()
        end = max(as_of.normalize(), data.index.max())
        calendar = pd.date_range(
            start=start,
            end=end,
            freq=self._policy.calendar_frequency,
            name="date",
        )

        data = data.reindex(calendar)
        data["symbol"] = symbol

        data = self._forward_fill(data, symbol)
        self._log_unfilled(symbol, data)

        data = self._derive_features(data)

        curated = data.reset_index()
        curated["symbol"] = symbol
        curated = curated[CANONICAL_COLUMNS]

        return curated

    def _apply_adjustments(self, frame: pd.DataFrame) -> pd.DataFrame:
        if not self._adjust_policy or "adj_close" not in frame.columns:
            frame["adj_close"] = frame["close"]
            return frame

        close = frame["close"].replace(0, np.nan)
        ratio = frame["adj_close"] / close
        ratio = ratio.replace([np.inf, -np.inf], np.nan).fillna(1.0)

        price_columns = ["open", "high", "low", "close"]
        frame[price_columns] = frame[price_columns].multiply(ratio, axis=0)
        frame["adj_close"] = frame["close"]

        return frame

    def _forward_fill(self, frame: pd.DataFrame, symbol: str) -> pd.DataFrame:
        limit = max(0, self._policy.forward_fill_limit)
        if limit == 0:
            filled = frame.copy()
        else:
            filled = frame.ffill(limit=limit)

        if "symbol" in filled.columns:
            filled["symbol"] = symbol

        return filled

    def _log_unfilled(self, symbol: str, frame: pd.DataFrame) -> None:
        missing_mask = frame["close"].isna()
        if missing_mask.any():
            missing_dates = frame.index[missing_mask]
            formatted = ", ".join(date.strftime("%Y-%m-%d") for date in missing_dates)
            logger.warning(
                "Missing close data for %s after forward fill limit=%s on: %s",
                symbol,
                self._policy.forward_fill_limit,
                formatted,
            )

    def _derive_features(self, frame: pd.DataFrame) -> pd.DataFrame:
        close = frame["close"]
        frame["sma_100"] = close.rolling(window=100, min_periods=1).mean()
        frame["sma_200"] = close.rolling(window=200, min_periods=1).mean()
        frame["ret_1d"] = close.pct_change(periods=1, fill_method=None)
        frame["ret_20d"] = close.pct_change(periods=20, fill_method=None)

        window = max(1, self._policy.rolling_peak_window)
        frame["rolling_peak"] = close.rolling(window=window, min_periods=1).max()

        return frame


def _normalize_timestamp(value: date | str | pd.Timestamp) -> pd.Timestamp:
    timestamp = pd.Timestamp(value)
    if timestamp.tzinfo is not None:
        timestamp = timestamp.tz_convert(None)
    return timestamp.normalize()


def _infer_symbol(path: Path, frame: pd.DataFrame) -> str:
    if "symbol" in frame.columns:
        unique = frame["symbol"].dropna().unique()
        if len(unique) == 1:
            return str(unique[0]).upper()
        if len(unique) > 1:
            raise ValueError(f"Multiple symbols detected in {path}: {unique!r}")
    return path.stem.upper()


__all__ = ["Preprocessor", "PreprocessResult"]
